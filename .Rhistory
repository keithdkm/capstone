n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog,encoding = "UTF-8"))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = BigramTokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = BigramTokenizer))
inspect(tdm)
head(tdm$Terms)
length(tr.blog)
head(tdm$Terms[1])
tdm$dimnames
tdm$dimnames[1]
head(tdm$dimnames[1])
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = BigramTokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3))
tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
head(tdm$i)
head(tdm$j)
a<-inspect(tdm)
View(a)
rm(a)
?frequency
?VCorpus
?NGramTokenizer
WOW(NGramTokenizer())
WOW(NGramTokenizer
)
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.002  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.01  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.05  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 3, delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))
blogs.inds      <-   sample(x      = length(allblogs),
size    = round(n* size* length(allblogs)),
replace = FALSE)
news.inds       <-   sample(x      = length(allnews),
size    = round(n* size* length(allnews)),
replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter),
size    = round(n* size* length(alltwitter)),
replace = FALSE)
tr.blog   <- allblogs[blogs.inds]
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]
corp.blog <- VCorpus(VectorSource(tr.blog))
Tokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 5, delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))
rm(tdm)
inspect(corp.blog)
corp.blog[[899]]
corp.blog$Content[[899]]
corp.blog[[899]]$content
tr.blogs[899]
blogs.inds[899]
?readLines
source('~/.active-rstudio-document', echo=TRUE)
tr.blogs[899]
tr.blog[899]
head(grep('[^0-9a-zA-Z/n/s/'/./"/$/%"]',allblogs))
head(grep('[^0-9a-zA-Z/n/s/'/./"/$/%]',allblogs))
)
head(grep('[^0-9a-zA-Z/n/s/'/"/$/%]',allblogs))
head(grep('[^0-9a-zA-Z/n/s/$/%]',allblogs))
allblogs[1]
head(grep('[^0-9a-zA-Z/n/s/$/%/"]',allblogs))
head(grep('[^0-9a-zA-Z/n/s/$/%/"/,]',allblogs))
allblogs[1]
head(grep('[^0-9a-zA-Z/n/s/$/%/",.]',allblogs))
allblogs[1]
?grep
head(grep('[^0-9a-zA-Z/n/s/$/%/",.]',allblogs, value = TRUE))
head(grep('[^0-9a-zA-Z/n/s/$/%/",.]',allblogs[1], value = TRUE))
source('~/.active-rstudio-document', echo=TRUE)
library("tm", lib.loc="~/R/win-library/3.2")
library("RWeka", lib.loc="~/R/win-library/3.2")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
?content_transformer
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
head(corp.blog)
corp.blog$1
corp.blog$`2`
corp.blog
tm_map(tr.blog, content_transformer(gsub))
tm_map(tr.blog, content_transformer(grep),'[0-9]')
?tm_map
tm_map(tr.blog, grep,'[0-9]')
source('~/.active-rstudio-document', echo=TRUE)
blog.tdm$tdm
a<-blog.tdm$dimnames
a<-blog.tdm$dimnames$Terms
a<-data.frame(blog.tdm$dimnames$Terms)
View(a)
a<-data.frame(token=blog.tdm$dimnames$Terms)
blog.tdm$j
blog.tdm$v
?options
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
paste0(tr.blog)
?paste0
paste0(tr.blog,collapse = TRUE)
paste(tr.blog,collapse = TRUE)
paste(tr.blog,collapse = "")
paste(tr.blog,collapse = " ")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
?tm_map
data("crude")
crude[[1]]
(f <- content_transformer(function(x, pattern) gsub(pattern, "", x)))
tm_map(crude, f, "[[:digit:]]+")[[1]]
crude
inspect(crude)
f<-content_transformer(function(x,pattern,new)
gsub(pattern,
new,
x))
grep("\"", tr.blog)
grep("\"", tm_map(tr.blog,f,"\"",""))
grep("\"", tm_map(tr.blog,f,".""))
)
grep(".", tm_map(tr.blog,f,".",""))
f
str(fun)
str(f)
clas(f)
class(f)
View(f)
class(tr.blog)
grep(".", tm_map(corp.blog,f,".",""))
tm_map(corp.blog,f,".","")
inspect(corp.blog)
corp.blog
inspect(corp.blog)
inspect(corp.blog$contect)
inspect(corp.blog$content)
corp.blog$content
corp.blog$content[[1]]
corp.blog$`1`$content
corp.blog$`1`
corp.blog
str(corp.blog)
corp.blog$`1`
corp.blog$1
corp.blog$[[1]]
corp.blog$[1]
attributes(corp.blog)
attributes(corp.blog)$names
attributes(corp.blog)$names$content
attributes(corp.blog)$names$'1'
attributes(corp.blog)$names$[1]
attributes(corp.blog)$names[1]
corp.blog$names[1]
corp.blog$content[1]
corp.blog$content[[1]]
corp.blog$content
class(corp.blog)
str(corp.blog)
corp.blog[[1]]
corp.blog[[1]]$content
corp.blog[[1]]$meta
corp.blog[[1]]$meta$author
corp.blog[[1]]$meta$datetimestamp
corp.blog[1]$meta$datetimestamp
corp.blog[[1]$meta$datetimestamp
corp.blog[[1]]$meta$datetimestamp
meta(corp.blog)
?meta
meta(tr.corpus[[1]])
meta(corp.blog[[1]])
?content_transformer
install.packages("packrat")
library("packrat", lib.loc="~/R/win-library/3.2")
packrat::init(options = list(external.packages = c("tm")))
library("tm", lib.loc="~/R/10. Capstone/packrat/lib-ext")
library("RWeka", lib.loc="~/R/10. Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
library("caret", lib.loc="~/R/10. Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
packrat::set_opts(vcs.ignore.src = TRUE, external.packages = c("tm", "caret", "e1071", "RWeka"))
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
?connection
source('~/R/Capstone/MakeSample.R')
?writeLines
?RDS
??RDS
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
?readRDS
setwd("~/R/Capstone")
tr.blog<-readRDS("Sample Data/blogsamp.RDS")
corp.blog <- VCorpus(VectorSource(tr.blog))
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Additional whitespace beyond a single space
# Remove digits
#
#
clean<-function(x){
x<-tm_map(x, stripWhitespace)
x<-tm_map(x, removePunctuation)
x<-tm_map(x, removeNumbers)
x<-tm_map(x, content_transformer(tolower))
f<-content_transformer(function(x,pattern,new)
gsub(pattern,
new,
x))
x<-tm_map(x,f,'[0-9]',"")
# x<-tm_map(x,gsub('[:;,-_#]',""))
}
corp.blog <- clean(corp.blog)
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Additional whitespace beyond a single space
# Remove digits
#
#
clean<-function(x){
x<-tm_map(x, stripWhitespace)
x<-tm_map(x, removePunctuation)
x<-tm_map(x, removeNumbers)
x<-tm_map(x, content_transformer(tolower))
f<-content_transformer(function(x,pattern,new)
gsub(pattern,
new,
x))
x<-tm_map(x,f,'\"',"")
# x<-tm_map(x,gsub('[:;,-_#]',""))
}
corp.blog <- clean(corp.blog)
corp.blog[[1]]$content
library("tm", lib.loc="~/R/Capstone/packrat/lib-ext")
source('~/.active-rstudio-document', echo=TRUE)
library("e1071", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
corpsample(1,0.1)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
source('~/.active-rstudio-document', echo=TRUE)
blog.tdm$dimnames
freqtable<-data.frame(tokens = blog.tdm$dimnames)
View(freqtable)
freqtable<-data.frame(tokens = blog.tdm$dimnames$Terms)
View(freqtable)
View(blog.tdm$i)
View(blog.tdm$j)
View(blog.tdm$num)
View(blog.tdm$v)
freqtable<-data.frame(Token = blog.tdm$dimnames$Terms, Frequency = blog.tdm$v)
View(freqtable)
a<-corp.blog[[1]]$content
View(a)
source('~/R/Capstone/MakeSample.R')
blogs.inds      <-   sample(x      = length(allblogs),
source('~/R/Capstone/MakeSample.R', echo=TRUE)
source('~/R/Capstone/MakeSample.R', echo=TRUE)
corpsample(1,0.1)
blogs.inds[1]
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
?removePunctuation
packrat::set_opts(external.packages = "")
install.packages("tm")
library("tm", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
?removePunctuation
getTransformations()
?removeWords
stopwords("en")
test<-tm_map(corp.blog,removeWords, stopwords(kind = "en"))
View(freqtable)
source('~/.active-rstudio-document', echo=TRUE)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
print("a")
source('~/R/Capstone/MakeSample.R')
source('~/.active-rstudio-document', echo=TRUE)
source('~/R/Capstone/MakeSample.R')
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("data.table")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
blog.tdm$dimnames[[1]])
blog.tdm$dimnames[[1]]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(freqtable)
corpsample(1,0.1)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
close(file"cleantext.txt")
close(file("cleantext.txt")
)
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
source('~/R/Capstone/MakeSample.R')
MakeSample(1,1)
source('~/R/Capstone/MakeSample.R')
MakeSample(1,1)
coprSample(1,1)
corpSample(1,1)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+ ',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
# # Remove certain characters
#   replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
#   x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
#   x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
#   x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
#   x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
#   x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
#   x<-tm_map(
clean.corp.blog <- clean(corp.blog)
writeLines(clean.corp.blog[[1]]$content, "cleantext.txt")
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
View(freqtable)
