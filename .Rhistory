corp.blog
inspect(corp.blog)
inspect(corp.blog$contect)
inspect(corp.blog$content)
corp.blog$content
corp.blog$content[[1]]
corp.blog$`1`$content
corp.blog$`1`
corp.blog
str(corp.blog)
corp.blog$`1`
corp.blog$1
corp.blog$[[1]]
corp.blog$[1]
attributes(corp.blog)
attributes(corp.blog)$names
attributes(corp.blog)$names$content
attributes(corp.blog)$names$'1'
attributes(corp.blog)$names$[1]
attributes(corp.blog)$names[1]
corp.blog$names[1]
corp.blog$content[1]
corp.blog$content[[1]]
corp.blog$content
class(corp.blog)
str(corp.blog)
corp.blog[[1]]
corp.blog[[1]]$content
corp.blog[[1]]$meta
corp.blog[[1]]$meta$author
corp.blog[[1]]$meta$datetimestamp
corp.blog[1]$meta$datetimestamp
corp.blog[[1]$meta$datetimestamp
corp.blog[[1]]$meta$datetimestamp
meta(corp.blog)
?meta
meta(tr.corpus[[1]])
meta(corp.blog[[1]])
?content_transformer
install.packages("packrat")
library("packrat", lib.loc="~/R/win-library/3.2")
packrat::init(options = list(external.packages = c("tm")))
library("tm", lib.loc="~/R/10. Capstone/packrat/lib-ext")
library("RWeka", lib.loc="~/R/10. Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
library("caret", lib.loc="~/R/10. Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
packrat::set_opts(vcs.ignore.src = TRUE, external.packages = c("tm", "caret", "e1071", "RWeka"))
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
?connection
source('~/R/Capstone/MakeSample.R')
?writeLines
?RDS
??RDS
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
corpsample(1, 1)
source('~/R/Capstone/MakeSample.R')
?readRDS
setwd("~/R/Capstone")
tr.blog<-readRDS("Sample Data/blogsamp.RDS")
corp.blog <- VCorpus(VectorSource(tr.blog))
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Additional whitespace beyond a single space
# Remove digits
#
#
clean<-function(x){
x<-tm_map(x, stripWhitespace)
x<-tm_map(x, removePunctuation)
x<-tm_map(x, removeNumbers)
x<-tm_map(x, content_transformer(tolower))
f<-content_transformer(function(x,pattern,new)
gsub(pattern,
new,
x))
x<-tm_map(x,f,'[0-9]',"")
# x<-tm_map(x,gsub('[:;,-_#]',""))
}
corp.blog <- clean(corp.blog)
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Additional whitespace beyond a single space
# Remove digits
#
#
clean<-function(x){
x<-tm_map(x, stripWhitespace)
x<-tm_map(x, removePunctuation)
x<-tm_map(x, removeNumbers)
x<-tm_map(x, content_transformer(tolower))
f<-content_transformer(function(x,pattern,new)
gsub(pattern,
new,
x))
x<-tm_map(x,f,'\"',"")
# x<-tm_map(x,gsub('[:;,-_#]',""))
}
corp.blog <- clean(corp.blog)
corp.blog[[1]]$content
library("tm", lib.loc="~/R/Capstone/packrat/lib-ext")
source('~/.active-rstudio-document', echo=TRUE)
library("e1071", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
corpsample(1,0.1)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
source('~/.active-rstudio-document', echo=TRUE)
blog.tdm$dimnames
freqtable<-data.frame(tokens = blog.tdm$dimnames)
View(freqtable)
freqtable<-data.frame(tokens = blog.tdm$dimnames$Terms)
View(freqtable)
View(blog.tdm$i)
View(blog.tdm$j)
View(blog.tdm$num)
View(blog.tdm$v)
freqtable<-data.frame(Token = blog.tdm$dimnames$Terms, Frequency = blog.tdm$v)
View(freqtable)
a<-corp.blog[[1]]$content
View(a)
source('~/R/Capstone/MakeSample.R')
blogs.inds      <-   sample(x      = length(allblogs),
source('~/R/Capstone/MakeSample.R', echo=TRUE)
source('~/R/Capstone/MakeSample.R', echo=TRUE)
corpsample(1,0.1)
blogs.inds[1]
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
?removePunctuation
packrat::set_opts(external.packages = "")
install.packages("tm")
library("tm", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
?removePunctuation
getTransformations()
?removeWords
stopwords("en")
test<-tm_map(corp.blog,removeWords, stopwords(kind = "en"))
View(freqtable)
source('~/.active-rstudio-document', echo=TRUE)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
print("a")
source('~/R/Capstone/MakeSample.R')
source('~/.active-rstudio-document', echo=TRUE)
source('~/R/Capstone/MakeSample.R')
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("data.table")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
blog.tdm$dimnames[[1]])
blog.tdm$dimnames[[1]]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(freqtable)
corpsample(1,0.1)
source('~/R/Capstone/MakeSample.R')
corpsample(1,0.1)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
close(file"cleantext.txt")
close(file("cleantext.txt")
)
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/i386-w64-mingw32/3.2.1")
source('~/R/Capstone/MakeSample.R')
MakeSample(1,1)
source('~/R/Capstone/MakeSample.R')
MakeSample(1,1)
coprSample(1,1)
corpSample(1,1)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+ ',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
# # Remove certain characters
#   replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
#   x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
#   x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
#   x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
#   x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
#   x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
#   x<-tm_map(
clean.corp.blog <- clean(corp.blog)
writeLines(clean.corp.blog[[1]]$content, "cleantext.txt")
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
View(freqtable)
source('~/R/Capstone/MakeSample.R')
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
?TermDocumentMatrix
View(freqtable)
?sort
data.table(?)
?data.table
?loop
?for
In
for
for (samp in 0:n-1) {
start = samp*size+1;
end   = start + size ; print c(start,end)}
for (samp in 0:n-1) {
start = samp*size+1;
end   = start + size ; print (c(start,end))}
n<-3; samp<-10
for (samp in 0:n-1) {
start = samp*size+1;
end   = start + size ; print (c(start,end))}
n<-3; size<-10
for (samp in 0:n-1) {
start = samp*size+1;
end   = start + size ; print (c(start,end))}
freqwords[grep("I am",freqwords[1,]),]
grep("I am",freqwords)
grep("I am",freqtable)
freqtable<-freqtable[freqtable$Frequency>1,]
grep("I am",freqtable$Token)
View(freqtable)
grep("i am",freqtable$Token)
grep("i am",freqtable$Token, value = T)
?max
grep("^i am",freqtable$Token))
grep("^i am",freqtable$Token)
grep("^i am",freqtable$Token,value =T)
freqtable(grep("^i am",freqtable$Token),"Frequency")
freqtable[grep("^i am",freqtable$Token),"Frequency"]
freqtable[grep("^i am",freqtable$Token),freqtable$Frequency]
grep("^i am",freqtable$Token)
freqtable[grep("^i am",freqtable$Token),]
max(freqtable[grep("^i am",freqtable$Token),])
max(freqtable[grep("^i am",freqtable$Token),3])
freqtable[grep("^i am",freqtable$Token),3]
?apply
?tapply
freqtable[1,3]
freqtable[2,3]
freqtable
freqtable[grep("^i am",freqtable$Token),Frequency]
freqtable[grep("^i am",freqtable$Token),Token]
freqtable[grep("^a case of",freqtable$Token),Token]
source('~/.active-rstudio-document', encoding = 'UTF-8', echo=TRUE)
n<-3; size<-10
for (samp in 0:(n-1)) {
start = samp*size+1;
end   = start + size ; print (c(start,end))}
for (samp in 0:(n-1)) {
start = samp*size+1;
end   = start + size -1; print (c(start,end))}
source('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
sampCorp(5,1)
corpSamp(5,1)
corpSample(5,1)
source('~/R/Capstone/MakeSample.R')
corpSample(5,1)
source('~/R/Capstone/MakeSample.R')
corpSample(5,1)
corpSample(5,1)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,1)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,1)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,1)
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
trblogs
tr.blogs
tr.blogs[samp]   <<- paste(allblogs[blogs.inds[(samp*blogsize + 1) :        (samp*blogsize + blogsize)]],collapse = " ")
trblogs[1:5]<-0
trblogs<-0
tr.blogs[samp]   <<- paste(allblogs[blogs.inds[(samp*blogsize + 1) :        (samp*blogsize + blogsize)]],collapse = " ")
tr.blogs   <<- paste(allblogs[blogs.inds[(samp*blogsize + 1) :        (samp*blogsize + blogsize)]],collapse = " ")
allblogs[blogs.inds[(samp*blogsize + 1) :        (samp*blogsize + blogsize)]]
blogs.inds[(samp*blogsize + 1) :        (samp*blogsize + blogsize)
]
blogs.inds[((samp-1)*blogsize + 1) :        ((samp-1)*blogsize + blogsize)
]
length(allblogs)
5*0.01*length(allb)
5*0.01*length(allblogs)
5*0.0001*length(allblogs)
twittersize = n* size* length(alltwitter))
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
source('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
class(tr.samp)
str(tr.samp)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(5,0.01)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(100,.01)
source('~/R/Capstone/MakeSample.R')
library("caret", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("e1071", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("tm",    lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("RWeka", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("stringi", lib.loc = "~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
setwd("~/R/Capstone")
options(mc.cores=4)
#Text is read in encoded as UTF-8
#Question 4 Number of lines in en_US.twitter where the word 'love' appears
# lhratio <-  length(grep("love", alltwitter, ignore.case = FALSE))/        length(grep("hate", alltwitter, ignore.case = FALSE))
# q5text<- alltwitter[grep('biostats',alltwitter)]
tr.samp<-readRDS("Sample Data/trsamp.RDS")  # Read in hte corpus of prepared samples
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
clean.corp.blog <- clean(corp.blog)
writeLines(clean.corp.blog[[1]]$content, "cleantext.txt")  #write out file for inspection
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
clean.corp.samp <- clean(corp.samp)
writeLines(clean.corp.samp[[1]]$content, "cleantext.txt")  #write out file for inspection
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
clean.tr.samp <- clean(tr.samp)
writeLines(clean.tr.samp[[1]]$content, "cleantext.txt")  #write out file for inspection
tr.samp$[[100]]
tr.samp[[100]]
tr.samp[[100]]$content
clean.tr.samp[[1:100]]$content
clean.tr.samp[1:100$content
clean.tr.samp[1:100]$content
Tokenizer <- function(x) NGramTokenizer(x,
Weka_control(min = 1,
max = 4,
delimiters =" .\n"))
blog.tdm <- TermDocumentMatrix(clean.tr.samp, control = list(tokenize = Tokenizer))
grep(".",blog.tdm$dimnames$Terms, value = T)
grep("\.",blog.tdm$dimnames$Terms, value = T)
grep("\n",blog.tdm$dimnames$Terms, value = T)
grep("[.]",blog.tdm$dimnames$Terms, value = T)
grep("[\n]",blog.tdm$dimnames$Terms, value = T)
clean.tr.samp[[3]]$meta
clean.tr.samp[[101]]$meta
clean.tr.samp[[100]]$meta
clean.tr.samp[[301]]$meta
clean.tr.samp[[300]]$content
clean.tr.samp[[100]]$content
tr.samp <-c(corp.blogs,corp.news,corp.twitter)
debugSource('~/R/Capstone/MakeSample.R')
corp(10,0.01)
corpSample(10,0.1)
corp.blogs[[1]]$meta
corp.blogs[[1:10]]$meta$origin = "blogs"
corp.blogs$meta$origin = "blogs"
corp.blogs[[1]]$meta
lapply(corp.blogs,function(x) {x$meta$origin = "blogs"})
corp.blogs[[1]]$meta
lapply(corp.blogs, "blogs")
lapply(corp.blogs, print)
lapply(corp.blogs, print(meta))
lapply(corp.blogs, print(meta$origin))
?meta
meta(corp.blogs, tag= origin) <- "blog"
debugSource('~/R/Capstone/MakeSample.R')
corpSample(10,0.1)
debugSource('~/R/Capstone/MakeSample.R')
corpSample(10,0.1)
meta(corp.blogs)
meta(corp.news)
tr.samp<-readRDS("Sample Data/trsamp.RDS")  # Read in hte corpus of prepared samples
clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove
# Remove any whitespace beyond a single space between words
# Remove any words appearing on Google's list of profane words
con<-file("Required Data/dirty.txt",'r')
profanity<-readLines(con)
close(con)
x<-tm_map(x,removeWords,profanity)
# Remove certain characters
replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions
x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}
clean.tr.samp <- clean(tr.samp)
writeLines(clean.tr.samp[[1:100]]$content, "cleantext.txt")  #write out file for inspection
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
Tokenizer <- function(x) NGramTokenizer(x,
Weka_control(min = 1,
max = 4,
delimiters =" .\n"))
blog.tdm <- TermDocumentMatrix(clean.tr.samp, control = list(tokenize = Tokenizer))
freqtable<-data.table(Token     = blog.tdm$dimnames[[1]], Length = stri_count_words(blog.tdm$dimnames[[1]]),
Frequency = blog.tdm$v)
freqtable<-freqtable[-grep('\n',freqtable$Token),]  #remove terms containing a new line i.e. token that span sentences
freqtable<-freqtable[order(freqtable$Length,freqtable$Token,freqtable$Frequency),]
freqwords<-findFreqTerms(blog.tdm,lowfreq=5)
freqtable<-freqtable[freqtable$Frequency>1,]  # remove ngrams with frequency of 1
blog.tdm$i==1
blgo.tdm[blog.tdm$i==1]$v
blog.tdm[blog.tdm$i==1]$v
blog.tdm[blog.tdm$i==1]
blog.tdm
aggregate(blog.tdm$v,list(blog$))
aggregate(blog.tdm$v,list(blog.tdm$i,blog.tdm$j),sum)
total_terms<-aggregate(blog.tdm$v,list(blog.tdm$i),sum)
View(total_terms)
total_term_count<-total_terms
total_terms<-aggregate(blog.tdm$v,list(blog.tdm$j),sum)
max(total_term_count$x)
total_term_count[max(total_term_count$x)),"Group.1"]
total_term_count[max(total_term_count$x),"Group.1"]
blog.tdm$dimnames$Terms[max(total_term_count$x)]
total_term_count[max(total_term_count$Group.1),"x"]
View(total_term_count)
blog.tdm$dimnames$Terms[1220465]
blog.tdm$dimnames$Terms[77658]
blog.tdm$dimnames$Terms[1520562]
blog.tdm$dimnames$Terms[438752]
total_term_count$term = blog.tdm$dimnames$Terms[total_term_count$Group.1]
total_term_count$Group.1<-NULL
saveRDS(total_term_count,"term_count")
