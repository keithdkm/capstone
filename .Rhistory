NGramTokenizer("on top")
TermDocumentMatrix(Corpus(VectorSource("on top")),control = NGramTokenizer()
)
TermDocumentMatrix(Corpus(VectorSource("on top")),control = list(tokenize = NGramTokenizer()))
View(dictionary)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
TermDocumentMatrix(Corpus(VectorSource("on top")),control = list(tokenize = Tokenizer_2())
)
TermDocumentMatrix(Corpus(VectorSource("on top")),control = list(tokenize = Tokenizer_2))
inspect(TermDocumentMatrix(Corpus(VectorSource("on top")),control = list(tokenize = Tokenizer_2)))
inspect(TermDocumentMatrix(Corpus(VectorSource("on top")),control = list(wordLengths=c(1,Inf),tokenize = Tokenizer_2)))
ngramfreq<-data.table(ngram="",frequency= 0, wordcount = 0, probability = 0)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_1 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
n<-500
size<-0.05
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(ngram="",frequency= 0, wordcount = 0, probability = 0)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_1 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =                                                                                                                                                 Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
View(final)
View(ngramfreq)
gc()
rm(dictionary)
rm(movies)
View(bigrams)
View(ngramfreq)
View(final)
rm(final)
saveRDS(ngramfreq,"Results/unigram 500 0.05 percent/ngramfreq.RDS")
View(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
setnames(ngramfreq,c("frequency","wordcount"), c("count","numwords"))
final<-ngramfreq[-1, count = sum(count) ,by = .(ngram)][, numwords := stri_count_words(ngram)]
final<-ngramfreq[-1, count := sum(count) ,by = ngram][, numwords := stri_count_words(ngram)]
View(final)
final<-ngramfreq[, count := sum(count) ,by = ngram][, numwords := stri_count_words(ngram)]
final<-ngramfreq[, count := sum(count) ,by = ngram][, numwords := stri_count_words(ngram)][-1,]
View(final)
rm(ngramfreq)
rm(final)
n<-1
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(ngram="", count = 0, numwords = 0, probability = 0)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_1 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
# final<-ngramfreq[-1, count = sum(count) ,by = ngram][, numwords = stri_count_words(ngram)]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(ngram="", count = 0)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_1 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
# final<-ngramfreq[-1, count = sum(count) ,by = ngram][, numwords = stri_count_words(ngram)]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
View(ngramfreq)
tabls()
tables()
test<-data.table(ngram, count)
unigrams<-ngramfreq[-1, count = sum(count) ,by = ngram][, numwords = stri_count_words(ngram)]
setkey(ngramfreq,ngram)
tables
[=:]
[]
[
function([)
tables()
setkey(bigrams,ngram)
bigram["good dog"]
bigrams["good dog"]
unigram["dog"]
ngramfreq["dog"]
ngramfreq[max(count)]
ngramfreq[count == max(count)]
ngramfreq[count == max(ngramfrq$count)]
ngramfreq[count == max(ngramfreq$count)]
unigrams<-ngramfreq[, sum(count) ,by = ngram][, numwords = stri_count_words(ngram)]
unigrams<-ngramfreq[, sum(count) ,by = ngram][, numwords := stri_count_words(ngram)]
View(unigrams)
unigrams<-ngramfreq[-1, count ,by = ngram][count := sum(count)][, numwords := stri_count_words(ngram)]
unigrams<-ngramfreq[-1, count ,by = ngram][,count := sum(count)][, numwords := stri_count_words(ngram)]
View(unigrams)
View(ngramfreq)
unigrams<-ngramfreq[-1, count:= sum(count) ,by = ngram][, numwords := stri_count_words(ngram)]
View(ngramfreq)
View(unigrams)
sum(ngramfreq$count)
inspect(readRDS("Sample Data/Sample Data 500x0.05/trsamp_1_0.05.RDS"))$content
inspect(readRDS("Sample Data/Sample Data 500x0.05/trsamp_1_0.05.RDS"))[[1]]$content
unigrams<-ngramfreq[-1, count:= sum(count) , by = ngram][, numwords := stri_count_words(ngram)][,probability = count/sum(count)]
unigrams<-ngramfreq[-1, count:= sum(count) , by = ngram][, numwords := stri_count_words(ngram)][,probability := count/sum(count)]
View(unigrams)
unigrams<-ngramfreq[-1, count:= sum(count) , by = ngram][, numwords := stri_count_words(ngram)][,probability := log10(count/sum(count))]
View(unigrams)
ngramfreq[-1,]
ngramfreq
unigrams<-ngramfreq[, count:= sum(count) , by = ngram][, numwords := stri_count_words(ngram)][-1,probability := log10(count/sum(count))]
View(unigrams)
View(bigrams)
setnames(unigrams,"ngrams","wordi")
View(unigrams)
setnames(unigrams,"ngram","wordi")
setnames(bigrams,"ngram","wordi")
View(bigrams)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(bigrams)
rm(bigrams)
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(wordi="", count = 0)
setkey(ngramfreq,wordi)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_2 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
# unigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)][,probability := log10(count/sum(count))]
#bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
View(ngramfreq)
gc()
tables()
setkey(ngramfreq,wordi)
tables()
library("stringi", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
stri_extract_last_words( ngramfreq[1,wordi])
stri_extract_last_words( ngramfreq[2,wordi])
stri_extract_words( ngramfreq[2,wordi])
stri_extract_all_words( ngramfreq[2,wordi])
stri_extract_all_words( ngramfreq[1:2,wordi])
stri_extract_all_words( ngramfreq[2:8,wordi])
unlist(stri_extract_all_words( ngramfreq[2:8,wordi]))
lapply(ngramfreq[2:8,wordi]),stri_extract_all_words)
lapply(ngramfreq[2:8,wordi],stri_extract_all_words)
sapply(ngramfreq[2:8,wordi],stri_extract_all_words)
vapply(ngramfreq[2:8,wordi],stri_extract_all_words)
?vapply
stri_extract_first_word( ngramfreq[2:8,wordi])
stri_extract_first_words( ngramfreq[2:8,wordi])
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ]
View(bigrams)
tables()
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count/unigrams[stri_extract_first_words(wordi),count])]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count/unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
View(bigrams)
unigram["bad",count]
unigrams["bad",count]
unigrams["babe",count]
log10(bigrams["a babe",count]/unigrams["a",count])
system.time(log10(bigrams["a babe",count]/unigrams["a",count]))
log10(bigrams["a babe",count])-log10(unigrams["a",count])
system.time(log10(bigrams["a babe",count])-log10(unigrams["a",count]))
system.time(log10(bigrams["lecture that",count])-log10(unigrams["lecture",count]))
log10(bigrams["lecture that",count])-log10(unigrams["lecture",count])
bigrams["lecture that",count]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
View(bigrams)
n<-50
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(wordi="", count = 0)
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_2 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
setkey(ngramfreq,wordi)
# unigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)][,probability := log10(count/sum(count))]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
View(bigrams)
View(ngramfreq)
View(bigrams)
View(ngramfreq)
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(wordi="", count = 0)
con<-file("Debug/sample_text.txt","rt")
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
writeLines(tr.samp[[1]]$content,con)
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_2 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
close(file)
setkey(ngramfreq,wordi)
# unigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)][,probability := log10(count/sum(count))]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(wordi="", count = 0)
con<-file("~/R/Capstone/Debug/sample_text.txt","rt")
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
writeLines(tr.samp[[1]]$content,con)
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_2 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
close(file)
setkey(ngramfreq,wordi)
# unigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)][,probability := log10(count/sum(count))]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
con<-file("~/R/Capstone/Debug/sample_text.txt","rt")
?file
con<-file("~/R/Capstone/Debug/sample_text.txt")
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.
#Aggregate ngram frequencies for all the samples
#
ngramfreq<-data.table(wordi="", count = 0)
con<-file("~/R/Capstone/Debug/sample_text.txt")
for (i in 1:n) {
file.name<-paste0("~/R/Capstone/Sample Data/Sample Data 500x0.05/trsamp_",i,"_",as.character(size),".RDS")
print(paste0("Reading ", file.name))
tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples
print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))
writeLines(tr.samp[[1]]$content,con)
tdm <- TermDocumentMatrix(tr.samp, control = list(wordLengths = c(1,Inf),tokenize = Tokenizer_2 )) ; rm(tr.samp)
ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm))
rm(tdm)
}
close(con)
setkey(ngramfreq,wordi)
# unigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi)][,probability := log10(count/sum(count))]
bigrams<-ngramfreq[, count:= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
# final<-ngramfreq[-1,.(Mean.Probability  =  sum(probability), Mean.Frequency = sum(frequency) ) ,by = .(ngram)][,":=" (  Mean.Probability =       Mean.Probability/n, Mean.Frequency = Mean.Frequency/n, wordcount = stri_count_words(ngram))]
bigrams<-ngramfreq[, count= sum(count) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
bigrams<-ngramfreq[, .(count= sum(count)) , by = wordi][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
View(bigrams)
bigrams<-ngramfreq[, .(count= sum(count)) , by = wordi][, numwords := stri_count_words(wordi)]
View(bigrams)
bigrams<-ngramfreq[, .(totalcount= sum(count)) , by = wordi][, numwords := stri_count_words(wordi)]
View(bigrams)
View(ngramfreq)
View(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
tables()
rm(ngramfreq)
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
gc()
gc()
View(unigrams)
gc()
gc()
makengramtable(50)
View(ngramfreq)
View(ngramfreq)
View(ngramfreq)
View(ngramfreq)
tr.samp<-readRDS("Sample Data/trsamp_1_0.05.RDS")
tr.samp<-readRDS("SampleSample Data/Sample Data 500x0.05/trsamp_1_0.05.RDS")
tr.samp<-readRDS("Sample Data/Sample Data 500x0.05/trsamp_1_0.05.RDS")
inspect(tr.samp)
inspect(tr.samp[[1]])
tr.samp[[1]]
tr.samp[[1]]$content
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
makengramtable(5)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
makengramtable(5)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
makengramtable(5)
View(bigrams)
tables()
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
50<0n
50<-n
n<-50
n<-5
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(ngramfreq)
ngramfreq[,]
ngram["a baby"]
ngramfreq["a baby"]
setkey(ngramfreq, "wordi")
ngramfreq["a baby"]
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(ngramfreq)
View(bigrams)
bigrams<-ngramfreq[, totalcount := sum(count) , by = .(wordi)][, numwords := stri_count_words(wordi), ][,pwiw1 := log10(totalcount)-log10(unigrams[stri_extract_first_words(ngramfreq$wordi),count])]
View(bigrams)
tables()
bigrams<-ngramfreq[, .(totalcount = sum(count)) , .(by = .(wordi))]
bigrams<-ngramfreq[, .(totalcount = sum(count)) , by = .(wordi)]
View(bigrams)
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(bigrams)
View(ngramfreq)
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
rm(bigrams)
rm(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(bigrams)
bigrams[,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(wordi),count])]
bigrams[,pwiw1 := log10(count)-log10(unigrams[stri_extract_first_words(bigrams$wordi),count])]
View(bigrams)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(unigrams)
gc()
debugSource('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(unigrams)
View(unigrams)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
packrat::snapshot(prompt = FALSE)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
library("stringi", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("stringr", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("stringr", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
n<-200
trigrams <- ngramfreq[numwords == 3, .(count = sum(count)) , by = .(wordi)][, c("W1", "W2","W3") := tstrsplit(wordi, " ", fixed = TRUE)]
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(trigrams)
View(bigrams)
?do.call
trigrams <- ngramfreq[numwords == 3, .(count = sum(count)) , by = .(wordi)][, c("W1", "W2","W3") := tstrsplit(wordi, " ", fixed = TRUE)][,W1W2:=paste0(W1,W2)]
View(trigrams)
?paste0
View(trigrams)
trigrams <- ngramfreq[numwords == 3, .(count = sum(count)) , by = .(wordi)][, c("W1", "W2","W3") := tstrsplit(wordi, " ", fixed = TRUE)][,W1W2:=paste0(W1," ",W2]
trigrams <- ngramfreq[numwords == 3, .(count = sum(count)) , by = .(wordi)][, c("W1", "W2","W3") := tstrsplit(wordi, " ", fixed = TRUE)][,W1W2:=paste0(W1," ",W2)]
View(trigrams)
setkey(bigrams,wordi)
trigrams[, pw3_w2w1 := log10(count/ bigrams[trigram$W1W2, count])]
trigrams[, pw3_w2w1 := log10(count/ bigrams[trigrams$W1W2, count])]
View(trigrams)
View(bigrams)
n<-5
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
started.at=proc.time()
Sys.sleep(1)
?Sys.sleep
cat("Finished in",time.take(started.at),"\n")
cat("Finished in",time.taken(started.at),"\n")
cat("Finished in",timetaken(started.at),"\n")
cat("Finished ",n,"samples in ",timetaken(started.at),"\n")
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(ngramfreq)
ngramfreq<-data.table(list(list()))
View(ngramfreq)
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(quadgrams)
trigram["a address boyd", ]
trigrams["a address boyd", ]
trigrams[quadgrams$W1W2W3,][1:5]
View(trigrams)
trigrams[quadgrams$W1W2W3,count][1:5]
n<-2
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(quadgrams)
View(trigrams)
View(quadgrams)
gc()
tables()
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(quadgrams)
remove.packages("devtools", lib="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
View(trigrams)
gc()
n<-300
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(quadgrams)
gc()
setwd("Results/quadgrams 300x0.05")
saveRDS(unigrams,"unigrams.RDS")
saveRDS(bigrams,"bigrams.RDS")
saveRDS(trigrams,"trigrams.RDS")
saveRDS(quadgrams."quadgrams.RDS")
saveRDS(quadgrams,"quadgrams.RDS")
quadgrams["would mean the",]
quadgrams["make me the",]
quadgrams["struggling but the",]
quadgrams["date at the",]
quadgrams['be on my']
quadgrams['be on my',sum(count)]
trigrams["be on my",]
trigrams[wordi == "be on my",]
bigrams[wordi = "be on",]
View(bigrams)
bigrams[wordi == "be on",]
trigrams [ "be on",]
trigrams [ "be on",sum(count)]
unigrams[,.N]
unigrams[,.N]+bigrams[,.N]
unigrams[,.N]+bigrams[,.N]+trigrams[,.N]
quadgrams[,.N]
.SDcols
quadgrams[,.SDcols]
.SD
quadgrams[,print(.SD)]
quadgrams[,print(.SD),by  = W1W2W3]
quadgrams[,.SD,by  = W1W2W3,.SDcols = list("W3W2W1", "W1")]
quadgrams[,.SD,by  = W1W2W3,.SDcols = list("W1W2W3", "W1")]
quadgrams[,.SD,by  = W1W2W3,.SDcols = c("W1W2W3", "W1")]
quagrams["on my way"]
quadgrams["on my way",]
quadgrams("on my way",sum (count))
quadgrams["on my way",sum (count)]
quadgrams["be on my",sum (count)]
quadgrams["be on my" ,][count==max(count),wordi]
quadgrams["the dog is" ,][count==max(count),wordi]
quadgrams["date at the" ,][count==max(count),wordi]
quadgrams["make me the" ,][count==max(count),wordi]
quadgrams["with his little" ,][count==max(count),wordi]
quadgrams["with his little" ,][count==head(count),wordi]
quadgrams["with his little" ,][count==head(count,10),wordi]
quadgrams["faith during the" ,][count==head(count,10),wordi]
quadgrams["you must be" ,][count==head(count,10),wordi]
quadgrams["you must be" ,][order(count),wordi]
quadgrams["you must be" ,][order(-count),wordi]
phrase <-  function(x) {quadgrams [as.character(x), ][[order(-count),wordi]]}
phrase("cat sat on ")
phrase <-  function(x) {quadgrams [as.character(x), ][[order(-count),wordi]}
phrase <-  function(x) quadgrams [as.character(x), ][[order(-count),wordi]
phrase <-  function(x) quadgrams [as.character(x), ][order(-count),wordi]
phrase <-  function(x) quadgrams [as.character(x), ][order(-count),wordi]
phrase("cat sat on")
phrase("a case of")
tables()
object.size(quadgrams)
gc()
corpSample(5000,0.01)
corpSample(5000,0.01)
10<-n
n<-10
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
View(quadgrams)
phrase("a case of")
gc()
n<-4000
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')
