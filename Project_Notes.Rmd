---
title: "Capstone Notes"
author: "Keith Miller"
date: "Friday, July 10, 2015"
output: html_document
---

##Task 0 Notes

###Tasks to accomplish

- Obtaining the data 
- Can you download the data and load/manipulate it in R?  
- Familiarizing yourself with NLP and text mining 
- Learn about the -basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

###Questions to consider

- What do the data look like?
- Where do the data come from?
- Can you think of any other data sources that might help you in this project?
- What are the common steps in natural language processing?
- What are some common issues in the analysis of text data?
- What is the relationship between NLP and the concepts you have learned in the Specialization?

```{r Libraries, warning=FALSE,message=FALSE}
library("caret", lib.loc="~/R/win-library/3.2")
library("e1071", lib.loc="~/R/win-library/3.2")
library("tm",    lib.loc="~/R/win-library/3.2")
library("RWeka", lib.loc="~/R/win-library/3.2")
# options(mc.cores=2)

```


##Goal
Create a prediction algorithm that, given a stream of text, will predict the next word that the user will type.  It should be presented as a web-enabled Shiny app with an accompanying slide deck explaining what it does and why it is needed.  

##Approach
We have been given three bodies of text which will form our corpora from three different sources types - blogs, news feeds and twitter feeds. They are in English with some foreign words and are in a raw text form.  Together they make up about 900,000 lines of text and 200,000,000 characters.  Similar bodies are also available in German, Russian and Finnish.  

##Load all of the text

I decided to read in all the data first and use R to explore the dataset.  My plan is to quickly build samples of each file.  Once the samples are built, I will discard the original data. 

One piece of manual clean up was performed on the en_US.news.txt file.  A SUB character was removed with a text editor.  It was located in line 77259 between the words 1 and gallons. 

```{r Read in all the Data, cache=FALSE, echo=FALSE}
#Text is read in encoded as UTF-8
setwd("C:/Users/Keith_2/Documents/R/10. Capstone")
allblogs<-readLines("Initial Dataset/final/en_US/en_US.blogs.txt",
                    n       = -1,
                    skipNul = TRUE, 
                    warn    = FALSE,
                    encoding= "UTF-8")
allnews<-readLines("Initial Dataset/final/en_US/en_US.news.txt", 
                    n       = -1,
                    skipNul = TRUE, 
                    warn    = FALSE ,
                    encoding= "UTF-8")
alltwitter<-readLines("Initial Dataset/final/en_US/en_US.twitter.txt",
                      n       = -1,
                      skipNul = TRUE, 
                      warn    = FALSE,
                      encoding= "UTF-8")

```
###Quiz 1 Answers
```{r  Calculations for Quiz 1 answers, cache=TRUE, echo=FALSE}

#Question 4 Number of lines in en_US.twitter where the word 'love' appears
lhratio <-  length(grep("love", alltwitter, ignore.case = FALSE))/        length(grep("hate", alltwitter, ignore.case = FALSE))
q5text<- alltwitter[grep('biostats',alltwitter)]


```

1. The en_US.blogs.txt file is `r round(file.size("Initial Dataset/final/en_US/en_US.blogs.txt")/10^6,0)` Mbytes. The en_US.news.txt file is `r round(file.size("Initial Dataset/final/en_US/en_US.news.txt")/10^6,0)` Mbytes. The en_US.twitter.txt file is `r round(file.size("Initial Dataset/final/en_US/en_US.twitter.txt")/10^6,0)` Mbytes
2. The en_US.blogs.txt has `r format(length(allblogs),big.mark=',')` lines. The en_US.news.txt has `r format(length(allnews),big.mark=',')` lines.  The en_US.twitter.txt has `r format(length(alltwitter),big.mark=',')` lines.
3. The maximum length of a line in the blog file is `r max(nchar(allblogs))` characters.  The maximum length of a line in the news file is `r max(nchar(allnews))` characters.  The maximum length of a line in the twitter file is `r max(nchar(alltwitter))` characters.  
4. The ratio of the number of times love appears to number of times hate appears in the twitter file is `r lhratio`.
5. The line of text from the twitter feed is "`r q5text`".
6. `r length(grep('A computer once beat me at chess, but it was no match for me at kickboxing',alltwitter))` lines have "A computer once beat me at chess, but it was no match for me at kickboxing" in them.

##Task 1 : Data acquisition and cleaning



###Sample the data
The datasets that we have available are large so for initial data cleaning, exploratory data analysis, statistical analysis and model development we shall use a small fraction of the data. 

####What sampling technique should we use?
I will build randomly generated vectors of indices.  For initial exploratory work just one sample will be generated from each file

```{r  Generate a sample, cache=TRUE}
set.seed(11051205)
n         <- 1  # sets the number of samples
size      <- 0.001  # what proportion of the file should the sample represent
print(paste("Producing ",n," Sample(s), each representing ", as.character(size*100)," percent of the full text"))
mn.char.blogs <- round(mean(nchar(allblogs)))  #calculate the mean number of characters in each line
mn.char.news    <- round(mean(nchar(allnews)))
mn.char.twitter <- round(mean(nchar(alltwitter)))


blogs.inds      <-   sample(x      = length(allblogs), 
                           size    = round(n* size* length(allblogs)), 
                           replace = FALSE)
news.inds       <-   sample(x      = length(allnews), 
                           size    = round(n* size* length(allnews)),
                           replace = FALSE)
twitter.inds    <-   sample(x      = length(alltwitter), 
                           size    = round(n* size* length(alltwitter)),
                           replace = FALSE)
 
tr.blog   <- paste(allblogs[blogs.inds],collapse = ".")
tr.news    <- allnews [news.inds]
tr.twitter <- alltwitter [twitter.inds]

corp.blog <- VCorpus(VectorSource(tr.blog))


```
## Benchmarking
Running the 5% **uncleaned** sample of the blog data through the tokenizer for 2grams and 3grams required 2Gb of memory, took about 5 minutes and generated `r format(length(blog.tdm$i))` tokens

Running the 0.1% **uncleaned** sample of the blog data through the tokenizer for 2,3,4,5 grams ran in about a minute and generated 129157 tokens.  

##Cleaning the data

```{r Clean data function}
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove 
# Additional whitespace beyond a single space
# Remove digits
# 
# 

clean<-function(x){
  x<-tm_map(x, stripWhitespace)
  x<-tm_map(x, removePunctuation)
  x<-tm_map(x, removeNumbers)
  x<-tm_map(x, content_transformer(tolower))
  f<-content_transformer(function(x,pattern,new) 
                                gsub(pattern,
                                     new,
                                     x))
  x<-tm_map(x,f,'[0-9]',"")
  # x<-tm_map(x,gsub('[:;,-_#]',""))
}

corp.blog <- clean(corp.blog)

```


- Remove foreign characters
- Identify sentences
- Remove proper nouns? 
- Remove punctuation except periods.  What about apostrophes? 
- Remove extra whitespace
- Remove numbers and replace with period or some other flag showing that the two words are NOT associated directly with one another
- How will I deal with contractions?  e.g. We'd = we would, It's = It is or It has
- Patronymics
- Remove single letters that are not I and a

##Tokenize the Data
```{r Tokenize the data}
#Take the cleaned corpus and Tokenize it

Tokenizer <- function(x) NGramTokenizer(x, 
                                        Weka_control(min = 2, 
                                                     max = 5, 
                                                     delimiters = "- \r\n\t.,;:'\"()?!"))
blog.tdm <- TermDocumentMatrix(corp.blog, control = list(tokenize = Tokenizer))



```



##Exploratory Data Analysis
- What is the distribution of line lengths?  Important if lines are going to be the sampling unit we use. 
- what is the variance of line lengths
- What is the distribution of sentence lengths
```{r  Line lengthe distributions}
par(mfrow = c(2,2))
hist(log10(nchar(allnews)))
hist(log10(nchar(allblogs)))
hist(log10(nchar(alltwitter)))

```
##Questions 
- Should we use stopwords ?  Which ones?
- Do we need to use stemmed words?   
- How do we break ties?
- Use a hashing function for fast searching of Ngrams
- Use RDS files for rapid file writing and reading
- Should lemmatization - probably not - loses context
- Should we alter contractions,  probably as "it's"" and "it is"" should predict the same thing but we don't want to predict a contracted item.  However if we do that then we need to translate contractions before predicting.  Problem - what happens if there's two possible expansions.
- Shoudl we predict paired capitalized words such as New York, El Paso Also possible that Capitalized words should be removed entirely.  May be removed by sparse filter from TDM
- Abbreviations? 


