---
title: "Capstone Notes"
author: "Keith Miller"
date: "Friday, July 10, 2015"
output: html_document
---

##Task 0 Notes

###Tasks to accomplish

- Obtaining the data 
- Can you download the data and load/manipulate it in R?  
- Familiarizing yourself with NLP and text mining 
- Learn about the -basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

###Questions to consider

- What do the data look like?
- Where do the data come from?
- Can you think of any other data sources that might help you in this project?
- What are the common steps in natural language processing?
- What are some common issues in the analysis of text data?
- What is the relationship between NLP and the concepts you have learned in the Specialization?

```{r Libraries, warning=FALSE,message=FALSE}
library("caret", lib.loc="~/R/win-library/3.2")
library("e1071", lib.loc="~/R/win-library/3.2")
library("tm",    lib.loc="~/R/win-library/3.2")
library("RWeka", lib.loc="~/R/win-library/3.2")
library("data.table", lib.loc = "~/R/win-library/3.2" )
library("stringi", lib.loc = "~/R/win-library/3.2" )
setwd("~/R/Capstone")

options(mc.cores=4)

```


##Goal
Create a prediction algorithm that, given a stream of text, will predict the next word that the user will type.  It should be presented as a web-enabled Shiny app with an accompanying slide deck explaining what it does and why it is needed.  

##Approach
We have been given three bodies of text which will form our corpora from three different sources types - blogs, news feeds and twitter feeds. They are in English with some foreign words and are in a raw text form.  Together they make up about 900,000 lines of text and 200,000,000 characters.  Similar bodies are also available in German, Russian and Finnish.  

##Load all of the text

I decided to read in all the data first and use R to explore the dataset.  My plan is to quickly build samples of each file.  Once the samples are built, I will discard the original data. 

One piece of manual clean up was performed on the en_US.news.txt file.  A SUB character was removed with a text editor.  It was located in line 77259 between the words 1 and gallons. 

```{r Read in all the Data, cache=FALSE, echo=FALSE}
#Text is read in encoded as UTF-8


```
###Quiz 1 Answers
```{r  Calculations for Quiz 1 answers, cache=TRUE, echo=FALSE}

#Question 4 Number of lines in en_US.twitter where the word 'love' appears
# lhratio <-  length(grep("love", alltwitter, ignore.case = FALSE))/        length(grep("hate", alltwitter, ignore.case = FALSE))
# q5text<- alltwitter[grep('biostats',alltwitter)]


```


##Task 1 : Data acquisition and cleaning



###Sample the data
The datasets that we have available are large so for initial data cleaning, exploratory data analysis, statistical analysis and model development we shall use a small fraction of the data.   The approach will be to take many  samples of several thousand lines.  We will then average the n-gram frequencies of these samples to approximate the popualtion frequency. 

####What sampling technique should we use?
I will build randomly generated vectors of indices.  For initial exploratory work just one sample will be generated from each file
```{r Read in prepared samples}


tr.blog<-readRDS("Sample Data/blogsamp.RDS")

corp.blog <- VCorpus(VectorSource(tr.blog))
```


###Cleaning the data

```{r Clean data function}

clean<-function(x){
# This function removes cleans the data stream to clean the data streams.  Intitially we wil remove 
# Remove any whitespace beyond a single space between words  

# Remove any words appearing on Google's list of profane words  
  con<-file("Required Data/dirty.txt",'r')
  profanity<-readLines(con)
  close(con)
  x<-tm_map(x,removeWords,profanity)

# Remove certain characters    
  replacechars<-content_transformer(function(x,pattern,new) gsub(pattern, new, x))
  x<-tm_map(x, replacechars, '[.?!]+[ ]',              "\n")  #replace all sentence ending chars with newline
  x<-tm_map(x, replacechars, '[\'\`]',      "" )  #remove apostrohes from contractions 
  x<-tm_map(x, replacechars, '[0-9()\"“”:;]', ".") #All other stop characters and numerics replace with a period
  x<-tm_map(x, replacechars, '[^a-zA-Z. \n]',         "")  #remove all other unknown chars
  x<-tm_map(x, replacechars, '[ ][^AaIi\n][ ]',       ".")  #Remove single letters that are not valid single letters
  x<-tm_map(x, replacechars, '[ ][ ]+', " ")  #remove extra whitespace
}

clean.corp.blog <- clean(corp.blog)

writeLines(clean.corp.blog[[1]]$content, "cleantext.txt")

```


- Remove foreign characters  Yes
- Identify sentences Yes
- Remove proper nouns-  no but they are converted to lower case
- Remove punctuation except periods.  - Periods at the end of sentences are replaced by newlines to maintain sntence.  All other chararcters are removed and replaced witha period 
- Remove extra whitespace
- Remove numbers and replace with period or some other flag showing that the two words are NOT associated directly with one another
- How will I deal with contractions?  e.g. We'd = we would, It's = It is or It has
- Patronymics
- Remove single letters that are not I and a
-

###Tokenize the Data
```{r Tokenize the data}
#Take the cleaned corpus and Tokenize it

Tokenizer <- function(x) NGramTokenizer(x, 
                                        Weka_control(min = 1, 
                                                     max = 3,
                                                     delimiters =" ."))
# delimiters =" -_|\n\t.,;:\'\"()?!"
blog.tdm <- TermDocumentMatrix(clean.corp.blog, control = list(tokenize = Tokenizer))

freqtable<-data.table(Token     = blog.tdm$dimnames[[1]], Length = stri_count_words(blog.tdm$dimnames[[1]]),
                      Frequency = blog.tdm$v)
freqtable<-freqtable[-grep('\n',freqtable$Token),]  #remove terms containing a new line i.c token that span sentences

freqtable<-freqtable[order(freqtable$Length,freqtable$Token,freqtable$Frequency),]

freqwords<-findFreqTerms(blog.tdm,lowfreq=5)
```

#### Benchmarking



##Task 2: Exploratory Data Analysis


###Question to Consider
1. Some words are more frequent than others - what are the distributions of word frequencies?
1. What are the frequencies of 2-grams and 3-grams in the dataset?
1. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
1. How do you evaluate how many of the words come from foreign languages?
1. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

### My issues  
- Should we use stopwords ?  Which ones?  Stopwords are being left in as you want to both predict stopwords and use them for prediction
- Do we need to use stemmed words?   No, words will be used as they are.
- How do we break ties?
- Use a hashing function for fast searching of Ngrams
- Use RDS files for rapid file writing and reading.  
- Should lemmatization - probably not - loses context
- Should we alter contractions,  probably as "it's"" and "it is"" should predict the same thing but we don't want to predict a contracted item.  However if we do that then we need to translate contractions before predicting.  Problem - what happens if there's two possible expansions.
- Shoudl we predict paired capitalized words such as New York, El Paso Also possible that Capitalized words should be removed entirely.  May be removed by sparse filter from TDM
- Abbreviations? 
