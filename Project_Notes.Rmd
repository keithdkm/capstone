---
title: "Capstone Notes"
author: "Keith Miller"
date: "Friday, July 10, 2015"
output: html_document
---

##Task 0 Notes

###Tasks to accomplish

- Obtaining the data 
- Can you download the data and load/manipulate it in R?  
- Familiarizing yourself with NLP and text mining 
- Learn about the -basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization.

###Questions to consider

- What do the data look like?
- Where do the data come from?
- Can you think of any other data sources that might help you in this project?
- What are the common steps in natural language processing?
- What are some common issues in the analysis of text data?
- What is the relationship between NLP and the concepts you have learned in the Specialization?

```{r Libraries, warning=FALSE,message=FALSE}
library("caret", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("e1071", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("tm",    lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
options( java.parameters = "-Xmx4g" )
library("RWeka", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("data.table", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("stringi", lib.loc = "~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("stringr", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("plyr", lib.loc="~/R/Capstone/packrat/lib/x86_64-w64-mingw32/3.2.1")
library("parallel", lib.loc="~/R/Capstone/packrat/lib-R")
source('~/R/Capstone/MakeSample.R', encoding = 'UTF-8')

setwd("~/R/Capstone")



```


##Goal
Create a prediction algorithm that, given a stream of text, will predict the next word that the user will type.  It should be presented as a web-enabled Shiny app with an accompanying slide deck explaining what it does and why it is needed.  

##Approach
We have been given three bodies of text which will form our corpora from three different sources types - blogs, news feeds and twitter feeds. They are in English with some foreign words and are in a raw text form.  Together they make up about 900,000 lines of text and 200,000,000 characters.  Similar bodies are also available in German, Russian and Finnish.  

##Load all of the text

I decided to read in all the data first and use R to explore the dataset.  My plan is to quickly build samples of each file.  Once the samples are built, I will discard the original data. 

One piece of manual clean up was performed on the en_US.news.txt file.  A SUB character was removed with a text editor.  It was located in line 77259 between the words 1 and gallons. 

```{r Load the data,echo=FALSE, results = "as.is"}

n<-20

size <- 0.05

corpSample(n,size)

```


##Task 1 : Data acquisition and cleaning


###Sample the data
The datasets that we have available are large so for initial data cleaning, exploratory data analysis, statistical analysis and model development we shall use a small fraction of the data.   The approach will be to take many  samples of several thousand lines.  Some experimentation will be required to see decide a good sampling schema.  My initaal goal is to have a sufficiently large sample that we can distinguish the relative probabilitiesof 90-95% of single words, not including English stopwords

####How do we decide on sample size
My approach to sampling will be to experiment with the number of samples and what proportion of the source text those samples represent.  I will build randomly generated vectors of indices to select random lines of text from all three source types, concatenate together all of the returned lines of text and store them as vectors in RDS files for easy re-use





###Cleaning the data


#### Data Cleaning Approach

The following cleaning steps were performed :
- Replace any characters that would "break" an n-gram run with a period so that the tokenizere won't identify the words on eother side of the character as as ngram
- Remove foreign characters  Yes
- Identify sentences Yes
- Remove proper nouns-  no but they are converted to lower case.  it is assumed that they will be infrequent enough not to matter.  
- Remove punctuation except periods.  - Periods at the end of sentences are replaced by newlines to maintain sentence structure so that ngrams that span sentences can be discarded.  All other chararcters are removed and replaced witha period 
- Remove extra whitespace Yes
- Remove numbers and replace with period or some other flag showing that the two words are NOT associated directly with one another Yes
- How will I deal with contractions?  e.g. We'd = we would, It's = It is or It has.  Contractions are handles by ignoring the apostrophe.  Text filter for the prediction algorithm must do the same
- Patronymics
- Remove single letters that are not I and a - Yes
-

###Tokenize the Data
```{r Tokenize the data}
#Take the cleaned corpus and Tokenize it into 1gram, 2grams and 3grams.


#Aggregate ngram frequencies for all the samples

#


ngramfreq<-data.table(ngram="",frequency= 0, wordcount = 0, probability = 0)

for (i in 1:n) {

print(paste("Processing Sample", i, " of ",size,"percent",Sys.time()))   
    
file.name<-paste0("Sample Data/trsamp_",i
                  ,"_",as.character(size),".RDS")
 
print(paste0("Reading ", file.name))

tr.samp<-readRDS(file.name)  # Read in hte corpus of prepared samples

tdm <- TermDocumentMatrix(tr.samp, control = list(tokenize = Tokenizer_3 )) ; rm(tr.samp)

ngramfreq<-rbind(ngramfreq,ngramcoverage(tdm)) 

rm(tdm)

# setkey(y,"ngram");

# ngramfreq <- merge (x = ngramfreq, y = y , by = "ngram", all = TRUE, suffixes =  )

# cols<-ncol(ngramfreq)
# 
# setnames(ngramfreq, c(cols-1,cols),c(paste("Probability",i),paste("Frequency",i)))


}


final<-ngramfreq[,.(Mean.Probability  =  mean(probability), Mean.Frequency = mean(frequency) ) ,by = .(ngram)][,wordcount := stri_count_words(ngram)]


```


##Task 2: Exploratory Data Analysis

###
1. Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora. 
2. Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

How do you evaluate how many of the words come from foreign languages? 
Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?



###Question to Consider
1. Some words are more frequent than others - what are the distributions of word frequencies?
1. What are the frequencies of 2-grams and 3-grams in the dataset?
1. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
1. How do you evaluate how many of the words come from foreign languages?
1. Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?

### My issues  

- Should we use stopwords ?  Which ones?  Stopwords are being left in as you want to both predict stopwords and use them for prediction
- Do we need to use stemmed words?   No, words will be used as they are.
- How do we break ties?
- Use a hashing function for fast searching of Ngrams
- Use RDS files for rapid file writing and reading.  
- Should lemmatization - probably not - loses context
- Should we alter contractions,  probably as "it's"" and "it is"" should predict the same thing but we don't want to predict a contracted item.  However if we do that then we need to translate contractions before predicting.  Problem - what happens if there's two possible expansions.
- Shoudl we predict paired capitalized words such as New York, El Paso Also possible that Capitalized words should be removed entirely.  May be removed by sparse filter from TDM
- Abbreviations? 

##Task 3 Modeling


The goal here is to build your first simple model for the relationship between words. This is the first step in building a predictive text mining application. You will explore simple models and discover more complicated modeling techniques.

###Tasks to accomplish

1. Build basic n-gram model - using the exploratory analysis you performed, build a basic n-gram model (http://en.wikipedia.org/wiki/N-gram) for predicting the next word based on the previous 1, 2, or 3 words.
2. Build a model to handle unseen n-grams - in some cases people will want to type a combination of words that does not appear in the corpora. Build a model to handle cases where a particular n-gram isn't observed.

###Questions to consider

1. How can you efficiently store an n-gram model (think Markov Chains)?
2. How can you use the knowledge about word frequencies to make your model smaller and more efficient?
3. How many parameters do you need (i.e. how big is n in your n-gram model)?
4. Can you think of simple ways to "smooth" the probabilities (think about giving all n-grams a non-zero probability even if they aren't observed in the data) ?
1. How do you evaluate whether your model is any good?
1. How can you use backoff models (http://en.wikipedia.org/wiki/Katz%27s_back-off_model)to estimate the probability of unobserved n-grams?
